
Current status:   

While formatted_transcript is generated the content is corrected to be in coherent sentences that use periods, 
commas, and capital letters as requirementts dictate.

The purpose of the LLM is to translate a raw audio transcript into an essay format that has paragraphs and gramatically correct 
sentences. Headings where appropriate.  Paragraphs and headings are separated by a blank line.  Do not use connecting dashes.  
Connecting dashes can be replaced with a comma followed by a space or a period followed by two spaces and the capital letter of 
a new sentence as appropriate if grammar cannot make the transition with an appropriate connecting word.

Current output is not formattted.  This does not meet requirements.  I seen no capital letters or punctation 
marks or paragraphs.  They should all be there.  A file showing sample output is 'desired_output.txt'

The document is split into multiple chunks with overlap making smooth transitions in the output. Each time a chunk is 
processed a command line entry should show a new call has been made to the llm witth the chunk.  The sample transcript is several chunks 
long but current output does not show that it is.

Dignose the problem.

Current output:

(venv) kdog@kdogsputer:~/Desktop/temp/process transcript$ python run.py
2025-06-21 19:11:42,470 - pipeline - INFO - Created 11 chunks
2025-06-21 19:11:42,471 - pipeline - INFO - Processing chunk 1/11
2025-06-21 19:11:42,471 - pipeline - INFO - LLM API call #1
2025-06-21 19:11:42,633 - pipeline - ERROR - LLM Error: Empty LLM response
2025-06-21 19:11:42,633 - pipeline - ERROR - Processing failed: Empty LLM response
2025-06-21 19:11:42,633 - root - ERROR - Fatal error: Empty LLM response
Traceback (most recent call last):
  File "/home/kdog/Desktop/temp/process transcript/run.py", line 33, in <module>
    asyncio.run(main())
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/home/kdog/Desktop/temp/process transcript/run.py", line 23, in main
    await pipeline.process_file(
  File "/home/kdog/Desktop/temp/process transcript/pipeline.py", line 123, in process_file
    formatted = await self.formatter.format_with_llm(combined)
  File "/home/kdog/Desktop/temp/process transcript/pipeline.py", line 58, in format_with_llm
    raise ValueError("Empty LLM response")
ValueError: Empty LLM response



Design :

Transistions are smooth because the chumking connects already formatted sections as it builds output.  
Overlap where fresh lines are added supplies a leading edge of formated overlap so the chunk being 
formatted can be made to ttransition correct so that the connection is later correct.

example.  If a chunk is 1000 characters long the second chunnk would begin at 800 characters so the 
first 200 characters are formattted with previous content.  This overlap allows the overlap to be 
rewritten when the second chunk overwrites the tail of the first chunk.

The end of the second chunk is at 1800 so the third chunk begins at 1600 to repeat the overlap 
re-write process until a smooth translation of the entire file has been accomplished.

This should be a common technique to process a translation in chunks and the web should be 
consulted as needed to ensure logic is correct.

Chunking will have to be along word boundaries because of the nature of the continuous transcript.   

The overlap is intended to fix problems with chunking in the middle of a sentence or paragraph.
