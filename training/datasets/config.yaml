base_model: /media/external_drive1/ai/textdata/models/TinyLlama-1.1B-Chat-v1.0
model_type: llama
tokenizer_type: AutoTokenizer
trust_remote_code: false

# QLoRA Configuration
adapter: qlora
load_in_4bit: true
use_qlora: true

# 4-bit Quantization
bnb_4bit_compute_dtype: float16
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true

# LoRA Parameters
lora_r: 64
lora_alpha: 128
lora_target_modules: ["q_proj", "v_proj"]
lora_dropout: 0.1

# Batch Configuration
micro_batch_size: 1
gradient_accumulation_steps: 8

# Training Schedule
max_steps: 600
learning_rate: 2.5e-5
lr_scheduler_type: cosine
warmup_ratio: 0.1
max_grad_norm: 0.5

# Dataset - UPDATED SECTION
datasets:
  - path: /home/kdog/pythonprojects/process_transcript/training/datasets/augmented_dataset.jsonl
    type: alpaca  # Changed from json to alpaca
    dataset_prepared_path: /home/kdog/pythonprojects/process_transcript/training/datasets/prepared

# System Configuration
fp16: true
gradient_checkpointing: true
optim: adamw_torch
weight_decay: 0.01

# Logging/Saving
save_strategy: steps
save_steps: 200
logging_steps: 20