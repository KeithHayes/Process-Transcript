
The task

async def formatchunk(self, chunktext: str) -> str:


Is to be rewritten.  The LoRA created from the training data will be used to format the input.  

The command is taken from the training data.

Training data is : /home/kdog/pythonprojects/process_transcript/training/datasets/augmented_dataset.jsonl

Checkpoints generated by axolotal using : /home/kdog/pythonprojects/process_transcript/training/datasets/config,yaml

is at : /home/kdog/text-generation-webui/training/trained_checkpoints/tinyllama_punctuation

kdog@kdogsputer:~/text-generation-webui/training/trained_checkpoints/tinyllama_punctuation$ ls
adapter_config.json        config.json              tokenizer.json
adapter_model.safetensors  README.md                tokenizer.model
chat_template.jinja        special_tokens_map.json
checkpoint-82              tokenizer_config.json
kdog@kdogsputer:~/text-generation-webui/training/trained_checkpoints/tinyllama_punctuation$ 

The text-generation-webui should be started with the LoRA if it is not yet started.  
A separate function should start the llm and load the LoRA as needed.